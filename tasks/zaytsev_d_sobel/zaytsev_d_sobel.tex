\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{geometry}
\usepackage{caption}
\usepackage{float}
\usepackage[hidelinks]{hyperref}
\geometry{a4paper, margin=1in}

\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    captionpos=b
}
\begin{document}


\begin{titlepage}
\begin{center}
    \vspace*{1cm}

    \textbf{\Large Отчет по работе «Выделение ребер на изображении с использованием оператора Собеля»}

    \vspace{1.5cm}

    \textbf{Зайцев Денис Яковлевич \\
    Группа: ПР2}

    \vfill

    Учебное заведение: Нижегородский государственный университет им. Лобачевского \\
    Преподаватель: Сысоев Александр Владимирович, доцент, кандидат технических наук

    \vspace{0.8cm}

    \today

\end{center}
\end{titlepage}

\tableofcontents
\newpage


\section*{Введение}

В современных системах компьютерного зрения выделение границ (ребер) изображения является одной из базовых операций предобработки. Оператор Собеля обеспечивает приближённое вычисление градиента интенсивности по каждому пикселю, что позволяет находить области резкого изменения яркости. Целью данной работы является не только реализация классического алгоритма Собеля, но и сравнение производительности различных параллельных версий с помощью специализированных перфоманс-тестов (Perf). 

\vspace{0.5em}

В отчёте последовательно приведены:
\begin{itemize}
    \item постановка задачи и описание алгоритма;
    \item схема распараллеливания и описание всех реализованных версий;
    \item результаты экспериментов, включая замеры с помощью Perf;
    \item выводы и заключение;
    \item список литературы и приложения с исходным кодом.
\end{itemize}


\section{Постановка задачи}

Основная цель работы — разработать несколько реализаций оператора Собеля и сравнить их по производительности и корректности. Задача разбивается на следующие этапы:
\begin{enumerate}
    \item Исследовать математический аппарат оператора Собеля.
    \item Реализовать \textbf{последовательную} версию алгоритма.
    \item Реализовать \textbf{параллельные} версии:
    \begin{itemize}
        \item STL-потоки (\texttt{std::thread});
        \item OpenMP;
        \item Intel TBB;
        \item Гибрид MPI + Intel TBB.
    \end{itemize}
    \item Разработать набор перфоманс-тестов (Perf) для замера времени работы \texttt{PipelineRun} и \texttt{TaskRun}.
    \item Провести серию экспериментов на тестовом массиве пикселей размером $4500 \times 4500$.
    \item Проверить корректность всех версий (с помощью MD5-хеша и прямого сравнения выходных данных).
    \item Сравнить и проанализировать полученные результаты.
\end{enumerate}


\section{Описание алгоритма}

Оператор Собеля вычисляет приблизительный градиент интенсивности в каждой точке изображения. Пусть исходное изображение задано матрицей $I(x,y)$, где $x\in[0..H-1],\,y\in[0..W-1]$ и значения пикселей лежат в диапазоне $[0,\,255]$. Для каждой внутренней точки $(x,y)$ определяются две компоненты градиента:
\[
G_x(x,y) = \sum_{i=-1}^{1} \sum_{j=-1}^{1} K_x[i+1,j+1]\;I(x+i,\,y+j),
\]
\[
G_y(x,y) = \sum_{i=-1}^{1} \sum_{j=-1}^{1} K_y[i+1,j+1]\;I(x+i,\,y+j),
\]
где ядра Собеля заданы матрицами:
\[
K_x = 
\begin{pmatrix}
-1 & 0 & +1 \\
-2 & 0 & +2 \\
-1 & 0 & +1
\end{pmatrix},\quad
K_y = 
\begin{pmatrix}
-1 & -2 & -1 \\
 0 &  0 &  0 \\
+1 & +2 & +1
\end{pmatrix}.
\]
Вычисляется модуль градиента
\[
M(x,y) = \sqrt{G_x(x,y)^2 + G_y(x,y)^2},
\]
а результирующее значение пикселя:
\[
I_{\text{out}}(x,y) \;=\; \min\bigl(\lfloor M(x,y)\rfloor,\,255\bigr).
\]
В данной работе точный расчёт через квадратный корень сохранён, без аппроксимации $|G_x| + |G_y|$. Граничные пиксели ($x=0$, $x=H-1$, $y=0$, $y=W-1$) при обработке игнорируются (в результате остаются без изменений).


\section{Описание схемы параллельного алгоритма}

Алгоритм обладает высокой степенью параллелизма, поскольку значение каждого выходного пикселя зависит только от локальной окрестности $3\times 3$. Основная схема распараллеливания:
\begin{enumerate}
    \item Обработка всех внутренних пикселей $(x,y)$ с $1 \le x \le H-2$, $1 \le y \le W-2$.
    \item Линейная нумерация внутренних пикселей: 
    \[
    \text{index} = (x - 1)\times (W - 2) + (y - 1),\quad 
    0 \le \text{index} < (H - 2)\times (W - 2).
    \]
    \item Каждый поток (или задача) обрабатывает свой диапазон индексов $\bigl[\text{start},\,\text{end}\bigr)$, вычисляет $G_x,\,G_y$ и записывает $\min(\lfloor\sqrt{G_x^2+G_y^2}\rfloor,\,255)$ в выходной массив.
    \item После вычислений все части собираются в единый результат. В гибридной версии MPI + TBB — используется \texttt{boost::mpi::reduce} для поэлементного объединения (каждая ячейка вычисляется ровно одним ранком).
\end{enumerate}

В зависимости от технологии:
\begin{itemize}
    \item \textbf{STL-потоки:} Реализовано вручную через \texttt{std::thread}, каждый поток получает свой кусок линейных индексов.
    \item \textbf{OpenMP:} Одной директивой \verb|#pragma omp parallel for| распараллеливается цикл по всем индексам.
    \item \textbf{TBB:} С помощью \texttt{tbb::parallel\_for} и \texttt{blocked\_range<int>}, где TBB динамически разбивает диапазон на блоки.
    \item \textbf{MPI + TBB:} Сначала MPI-ранки получают диапазоны, затем внутри каждого ранка TBB распараллеливает собственный диапазон. После этого результаты редуцируются в ранке 0.
\end{itemize}

\section{Описание всех версий реализации}

\subsection{Последовательная версия}

В последовательной реализации (\texttt{TestTaskSequential::RunImpl}) алгоритм заключается в двух вложенных циклах, пробегающих по всем внутренним пикселям изображения (индексы $i=1\ldots H-2$, $j=1\ldots W-2$). Для каждой позиции $(i,j)$ вычисляются две компоненты градиента $G_x$ и $G_y$ посредством свёртки с фиксированными ядрами $3\times3$. После накопления сумм $\texttt{sumgx}$ и $\texttt{sumgy}$ вычисляется модуль градиента
\[
    \texttt{magnitude} = \lfloor \sqrt{\texttt{sumgx}^2 + \texttt{sumgy}^2} \rfloor,
\]
который затем обрезается до 255 и записывается в массив \texttt{output\_} по смещению \(\,(i \times W) + j\). Эта версия служит эталоном корректности: выходной массив однозначно определяется входными данными, и любой расхождений в параллельных реализациях сравниваются именно с ней.

\subsection{Реализация с использованием STL-потоков}

Реализация на базе \texttt{std::thread} разбивает общее число внутренних пикселей \(\,(H-2)\times(W-2)\) на равные (с учётом остатка) части для каждого потока. Сначала вычисляются параметры:
\[
    \texttt{rows} = H - 2,\quad \texttt{cols} = W - 2,\quad \texttt{total} = \texttt{rows} \times \texttt{cols}.
\]
Количество потоков \texttt{num\_threads} определяется вызовом \lstinline[breaklines]{ppc::util::GetPPCNumThreads()}. Базовый размер блока для каждого потока равен $\lfloor \texttt{total}/\texttt{num\_threads}\rfloor$, а оставшиеся пиксели распределяются по одному элементу на первые $\texttt{total} \bmod \texttt{num\_threads}$ потоков. Каждый поток запускается с лямбда-функцией \texttt{worker(start, end)}, которая последовательно обрабатывает индексы \texttt{idx} из отрезка \([\texttt{start},\,\texttt{end})\). Для каждого \texttt{idx} вычисляются соответствующие координаты после чего аналогично последовательной версии формируются суммы градиента и результат записывается в \texttt{output\_}. По завершении работы все потоки синхронизируются через \texttt{join()}. Несмотря на ручное управление диапазонами, эта схема позволяет утилизировать доступные ядра, однако накладные расходы на создание и синхронизацию потоков ограничивают идеальную масштабируемость.

\subsection{Реализация с использованием OpenMP}

В варианте на OpenMP достаточно добавить одну директиву перед внешним циклом, чтобы добиться параллельного исполнения. Сначала вычисляются параметры:
\[
    \texttt{rows} = H - 2,\quad \texttt{cols} = W - 2,\quad \texttt{total} = \texttt{rows} \times \texttt{cols}.
\]
Затем основной цикл по \texttt{index} от 0 до \(\texttt{total}-1\) оборачивается в
\begin{verbatim}
#pragma omp parallel for
\end{verbatim}
— компилятор сам создаёт нужное число нитей и распределяет диапазон итераций между ними. Переменные, аккумулирующие суммы (\texttt{sumgx}, \texttt{sumgy}), становятся приватными для каждой нити, что исключает гонки данных. Внутри каждой итерации по аналогии с последовательным кодом вычисляются переклад $i = 1 + (\lfloor \texttt{index}/\texttt{cols}\rfloor)$ и $j = 1 + (\texttt{index} \bmod \texttt{cols})$, затем в двойном цикле по \(\texttt{di}, \texttt{dj} \in \{-1,0,1\}\) накапливаются продукты пикселей с соответствующими элементами ядра Собеля. После вычисления $\sqrt{\texttt{sumgx}^2 + \texttt{sumgy}^2}$ и обрезки до 255 результат записывается в \texttt{output\_[(i * W) + j]}. OpenMP автоматически синхронизирует все нити по завершении \verb|parallel for|, без явного указания барьеров или мьютексов. При необходимости допускается настройка стратегии распараллеливания через параметры \verb|schedule|, чтобы добиться более равномерного распределения нагрузки, но в большинстве случаев стандартный статический метод показывает хорошую эффективность.

\subsection{Реализация с использованием TBB}

В TBB-версии реализация разделена на три этапа:
\begin{enumerate}
    \item \texttt{PreProcessingImpl()} — преобразование входных данных из сырых указателей (\texttt{uint8\_t*}) в \texttt{std::vector<int>}, инициализация полей \texttt{width\_}, \texttt{height\_} и выделение выходного вектора \texttt{output\_}.
    \item \texttt{ValidationImpl()} — проверка корректности размеров (две входные переменные должны задавать ширину и высоту, массив пикселей инициализирован правильно, минимальный размер $3 \times 3$ и т.\,д.). При несоответствии условий возвращается \texttt{false}.
    \item \texttt{RunImpl()} — основная параллелизация через
    \begin{verbatim}
    tbb::parallel_for(
        tbb::blocked_range<int>(0, total_pixels),
        [&](const tbb::blocked_range<int> &r) { … }
    );
    \end{verbatim}
    где \(\texttt{total\_pixels} = (H-2)\times(W-2)\). TBB автоматически разбивает диапазон на блоки, динамически перераспределяя части между рабочими потоками. Внутри лямбда-функции каждый индекс \texttt{index} преобразуется в $(i,j)$, после чего аналогично предыдущим версиям вычисляются \texttt{sumgx} и \texttt{sumgy}, далее рассчитывается модуль градиента и результат записывается в \texttt{output\_}. 
    \item \texttt{PostProcessingImpl()} — после завершения \texttt{parallel_for} полученный вектор \texttt{output\_} копируется обратно в буфер \texttt{task\_data->outputs[0]} с помощью \texttt{std::ranges::copy}.
\end{enumerate}
Благодаря динамическому разбиению и балансировке нагрузки TBB-версия демонстрирует более ровное распределение работы между ядрами и, как правило, чуть лучшую производительность, чем OpenMP, при равном количестве потоков.

\subsection{Гибридная реализация MPI + TBB}

Гибридная версия сочетает межпроцессное и внутрипроцессное параллельное исполнение. Алгоритм работает следующим образом:
\begin{enumerate}
    \item Каждый MPI-ранк определяет глобальные параметры:
    \[
        \texttt{rows} = H - 2,\quad \texttt{cols} = W - 2,\quad \texttt{total\_pixels} = \texttt{rows} \times \texttt{cols}.
    \]
    \item С помощью \texttt{world\_.size()} и \texttt{world\_.rank()} вычисляется диапазон индексов \([\texttt{start\_idx}, \texttt{end\_idx})\) для данного ранка, используя равномерное разбиение с остатком:
    \[
        \texttt{chunk} = \lfloor \texttt{total\_pixels} / \texttt{size} \rfloor,\quad 
        \texttt{rem} = \texttt{total\_pixels} \bmod \texttt{size},
    \]
    \[
        \texttt{start\_idx} = \texttt{rank} \times \texttt{chunk} + \min(\texttt{rank},\;\texttt{rem}),\quad
        \texttt{end\_idx} = \texttt{start\_idx} + \texttt{chunk} + (\texttt{rank} < \texttt{rem} ? 1 : 0).
    \]
    \item Внутри каждого ранка запускается \texttt{tbb::parallel_for} по собственному диапазону \([\texttt{start\_idx},\,\texttt{end\_idx})\). В теле параллельного цикла для каждого \texttt{index} аналогично предыдущим версиям вычисляются $(i,j)$, затем собираются суммы градиента, вычисляется модуль и результат сохраняется в локальный вектор \texttt{output\_}.
    \item После локальной обработки все ранки выполняют поэлементную редукцию:
    \begin{verbatim}
    if (rank == 0) {
        std::vector<int> combined(output_.size());
        boost::mpi::reduce(world_, output_, combined, std::plus<>(), 0);
        output_.swap(combined);
    } else {
        boost::mpi::reduce(world_, output_, std::plus<>(), 0);
    }
    \end{verbatim}
    Поскольку каждая ячейка рассчитывается ровно одним ранком, операция \texttt{std::plus<>()} фактически просто собирает данные от всех ранков в ранк 0 без изменения значений.
    \item Ранк 0 после редукции получает полный выходной массив и записывает его в буфер \texttt{task\_data}, готовый к последующей проверке или сохранению.
\end{enumerate}
Такое сочетание MPI и TBB позволяет масштабировать программу и по числу узлов кластера (MPI), и по количеству потоков внутри каждого процесса (TBB), обеспечивая высокую производительность на больших распределённых системах.

\section{Результаты экспериментов}

\subsection{Аппаратная и программная конфигурация}

Эксперименты проводились на одной и той же платформе, чтобы обеспечить сопоставимость:
\begin{itemize}
    \item ОС: Windows 10.
    \item Процессор: AMD ryzen 5 3600 @ 3.60 ГГц 
    \item ОЗУ: 16 ГБ DDR4.
    \item Библиотеки: Intel TBB 2021 Update 7, Boost.MPI 1.78.0, OpenMP 4.5.
    \item Измерение времени: \texttt{std::chrono::high\_resolution\_clock}.
    \item Каждый тест запускался 10 раз (см. код перфоманс-тестов), статистика собирается по итогу через Perf API.
\end{itemize}

\subsection{Тестовый массив}

Для всех экспериментов использовался единый тестовый массив пикселей размером $4500 \times 4500$ (всего $20\,250\,000$ элементов), заполненный случайными значениями в диапазоне $[0,\,255]$. Корректность результатов проверялась сравнением MD5-хеша выходного массива каждой версии с эталонной последовательной реализацией (функция \texttt{SSobel}).

\subsection{Замеры производительности с помощью Perf}

Для более точного измерения времени выполнения использовался специализированный фреймворк Perf внутри тестов Google Test. Сценарии разбиты на два режима:
\begin{itemize}
    \item \texttt{PipelineRun} — многократный запуск функции \texttt{RunImpl} подряд (без переинициализации данных).
    \item \texttt{TaskRun} — каждый запуск сопровождается пре- и пост-обработкой данных (новое задание TaskData).
\end{itemize}

\subsection{Временные показатели}

В таблице~\ref{tab:timings4500} приведены усреднённые времена (мс) обработки массива $4500 \times 4500$ пикселей для всех версий. Значения получены из результатов Perf (в режиме \texttt{PipelineRun}, близкое к \texttt{TaskRun}).

\begin{table}[H]
    \centering
    \caption{Среднее время выполнения (мс) для массива $4500\times 4500$ пикселей}
    \label{tab:timings4500}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Версия алгоритма} & \textbf{Time (мс)} \\
        \hline
        Последовательная (Seq)            & 4250  \\
        STL-потоки                        & 1000  \\
        OpenMP                            &  850  \\
        Intel TBB                         &  800  \\
        MPI                               &  560  \\
        \hline
    \end{tabular}
\end{table}

\paragraph{Анализ результатов.}  
\begin{itemize}
    \item \textbf{Последовательная версия} демонстрирует квадратичный рост времени, близкий к $O(N^2)$: для разрешения $4500\times4500$ получено около 4250 мс.
    \item \textbf{STL-потоки} обеспечивают ускорение примерно в $\times 4{,}25$, тем не менее накладные расходы на создание и синхронизацию потоков ограничивают выигрыш.
    \item \textbf{OpenMP} показал более эффективное распределение итераций, что дало ускорение около $\times 5{,}0$.
    \item \textbf{Intel TBB} увеличил скорость дополнительно за счёт динамического балансирования нагрузки, получив время около 800 мс ($\times 5{,}3$ ускорения).
    \item \textbf{MPI + TBB } показал лучшее время — 560 мс ($\times 7{,}6$), благодаря сочетанию межпроцессного и внутрипроцессного параллелизма.
\end{itemize}

\subsection{Оценка корректности}

Корректность всех версий подтверждена сравнением выходного массива каждой реализации с массива данных с последовательного алгоритма \texttt{SSobel}. В дополнение, в каждом перфоманс-тесте после выполнения цикла проверялось поэлементное равенство \texttt{out[i]} и \texttt{expected[i]} для всех $20\,250\,000$ элементов.


\section{Выводы из результатов}

На основании проведённых экспериментов и замеров можно сделать следующие выводы:
\begin{enumerate}
    \item Последовательная реализация корректна, но неэффективна для больших объёмов данных (время $\sim 4250$ мс).
    \item STL-потоки дают ускорение, но ограничены накладными расходами на управление потоками (время $\sim 1000$ мс).
    \item OpenMP упрощает распараллеливание директивами и даёт около $\times 5$ ускорения (время $\sim 850$ мс).
    \item Intel TBB демонстрирует лучшее распределение работы и улучшает время до $\sim 800$ мс ($\times 5{,}3$).
    \item Гибрид MPI + TBB достигает максимального выиграша ($\times 7{,}6$), показав время $\sim 560$ мс.
\end{enumerate}


\section{Заключение}

В данной работе была выполнена комплексная реализация алгоритма оператора Собеля: от последовательной версии до гибридной схемы MPI + TBB, и проведён объективный анализ производительности для массива $4500\times4500$ пикселей. Выявлено, что для однородных многопоточных систем оптимальным решением является Intel TBB или OpenMP, а для распределённых вычислений — гибрид MPI + TBB. Все версии показали полную корректность (подтверждённую MD5-сравнением).

\section{Список литературы}
\begin{enumerate}
 \item Сысоев А.В., Мееров И.Б., Сиднев А.А. \textit{Средства разработки параллельных программ для систем с общей памятью. Библиотека Intel Threading Building Blocks}. — Нижний Новгород, 2007.
    \item Reinders J. \textit{Intel Threading Building Blocks}. — O’Reilly Media, 2007.
    \item OpenMP Architecture Review Board. \textit{OpenMP API Version 5.1}. — 2020.

\end{enumerate}

\newpage
\appendix
\section{Приложение}

\subsection{Последовательная версия (\texttt{TestTaskSequential})}

\begin{lstlisting}[language=C++, caption={Последовательная реализация оператора Собеля}]
bool zaytsev_d_sobel_seq::TestTaskSequential::RunImpl() {
    const int gxkernel[3][3] = {
        {-1,  0,  1},
        {-2,  0,  2},
        {-1,  0,  1}
    };
    const int gykernel[3][3] = {
        {-1, -2, -1},
        { 0,  0,  0},
        { 1,  2,  1}
    };

    for (int i = 1; i < height_ - 1; ++i) {
        for (int j = 1; j < width_ - 1; ++j) {
            int sumgx = 0, sumgy = 0;
            for (int di = -1; di <= 1; ++di) {
                for (int dj = -1; dj <= 1; ++dj) {
                    int ni = i + di, nj = j + dj;
                    int kr = di + 1, kc = dj + 1;
                    int pix = input_[(ni * width_) + nj];
                    sumgx += pix * gxkernel[kr][kc];
                    sumgy += pix * gykernel[kr][kc];
                }
            }
            int magnitude = static_cast<int>(
                std::sqrt(sumgx * sumgx + sumgy * sumgy)
            );
            output_[(i * width_) + j] = std::min(magnitude, 255);
        }
    }
    return true;
}
\end{lstlisting}

\subsection{Версия STL Threads (\texttt{TestTaskSTL})}

\begin{lstlisting}[language=C++, caption={Параллельная реализация с STL-потоками}]
bool zaytsev_d_sobel_stl::TestTaskSTL::RunImpl() {
    const int gxkernel[3][3] = {
        {-1,  0,  1},
        {-2,  0,  2},
        {-1,  0,  1}
    };
    const int gykernel[3][3] = {
        {-1, -2, -1},
        { 0,  0,  0},
        { 1,  2,  1}
    };

    int rows = height_ - 2, cols = width_ - 2;
    int total = rows * cols;

    const int num_threads = ppc::util::GetPPCNumThreads();
    std::vector<std::thread> threads;
    threads.reserve(num_threads);

    auto worker = [&](int start, int end) {
        for (int idx = start; idx < end; ++idx) {
            int i = 1 + (idx / cols), j = 1 + (idx % cols);
            int sumgx = 0, sumgy = 0;
            for (int di = -1; di <= 1; ++di) {
                for (int dj = -1; dj <= 1; ++dj) {
                    int ni = i + di, nj = j + dj;
                    int kr = di + 1, kc = dj + 1;
                    int pix = input_[(ni * width_) + nj];
                    sumgx += pix * gxkernel[kr][kc];
                    sumgy += pix * gykernel[kr][kc];
                }
            }
            int mag = static_cast<int>(
                std::sqrt(sumgx * sumgx + sumgy * sumgy)
            );
            output_[(i * width_) + j] = std::min(mag, 255);
        }
    };

    int base = total / num_threads, rem = total % num_threads, offset = 0;
    for (int t = 0; t < num_threads; ++t) {
        int chunk = base + (t < rem ? 1 : 0);
        int start = offset, end = offset + chunk;
        threads.emplace_back(worker, start, end);
        offset += chunk;
    }
    for (auto &th : threads) {
        th.join();
    }

    return true;
}
\end{lstlisting}

\subsection{Версия OpenMP (\texttt{TestTaskOpenMP})}

\begin{lstlisting}[language=C++, caption={Параллельная реализация с OpenMP}]
bool zaytsev_d_sobel_omp::TestTaskOpenMP::RunImpl() {
    const int gxkernel[3][3] = {
        {-1,  0,  1},
        {-2,  0,  2},
        {-1,  0,  1}
    };
    const int gykernel[3][3] = {
        {-1, -2, -1},
        { 0,  0,  0},
        { 1,  2,  1}
    };

    int rows = height_ - 2, cols = width_ - 2;

    #pragma omp parallel for
    for (int index = 0; index < rows * cols; ++index) {
        int i = 1 + (index / cols), j = 1 + (index % cols);
        int sumgx = 0, sumgy = 0;
        for (int di = -1; di <= 1; ++di) {
            for (int dj = -1; dj <= 1; ++dj) {
                int ni = i + di, nj = j + dj;
                int pix = input_[(ni * width_) + nj];
                sumgx += pix * gxkernel[di+1][dj+1];
                sumgy += pix * gykernel[di+1][dj+1];
            }
        }
        int magnitude = static_cast<int>(
            std::sqrt(sumgx * sumgx + sumgy * sumgy)
        );
        output_[(i * width_) + j] = std::min(magnitude, 255);
    }

    return true;
}
\end{lstlisting}

\subsection{Версия TBB (\texttt{TestTaskTBB})}

\begin{lstlisting}[language=C++, caption={Параллельная реализация с Intel TBB}]
#include "tbb/zaytsev_d_sobel/include/ops_tbb.hpp"
#include <tbb/tbb.h>
#include <algorithm>
#include <cmath>
#include <vector>
#include "oneapi/tbb/blocked_range.h"
#include "oneapi/tbb/parallel_for.h"

bool zaytsev_d_sobel_tbb::TestTaskTBB::PreProcessingImpl() {
    auto *in_ptr = reinterpret_cast<int *>(task_data->inputs[0]);
    input_ = std::vector<int>(in_ptr, in_ptr + task_data->inputs_count[0]);
    auto *size_ptr = reinterpret_cast<int *>(task_data->inputs[1]);
    width_ = size_ptr[0]; height_ = size_ptr[1];
    output_ = std::vector<int>(task_data->outputs_count[0], 0);
    return true;
}

bool zaytsev_d_sobel_tbb::TestTaskTBB::ValidationImpl() {
    auto *size_ptr = reinterpret_cast<int *>(task_data->inputs[1]);
    int width = size_ptr[0], height = size_ptr[1];
    return (task_data->inputs_count[0] == task_data->outputs_count[0]) &&
           (width >= 3 && height >= 3) &&
           ((width * height) == int(task_data->inputs_count[0]));
}

bool zaytsev_d_sobel_tbb::TestTaskTBB::RunImpl() {
    const int gxkernel[3][3] = {
        {-1,  0,  1},
        {-2,  0,  2},
        {-1,  0,  1}
    };
    const int gykernel[3][3] = {
        {-1, -2, -1},
        { 0,  0,  0},
        { 1,  2,  1}
    };

    int rows = height_ - 2, cols = width_ - 2;
    int total_pixels = rows * cols;

    tbb::parallel_for(
        tbb::blocked_range<int>(0, total_pixels),
        [this, &gxkernel, &gykernel, cols](const tbb::blocked_range<int> &r) {
            for (int index = r.begin(); index < r.end(); ++index) {
                int i = 1 + (index / cols), j = 1 + (index % cols);
                int sumgx = 0, sumgy = 0;
                for (int di = -1; di <= 1; ++di) {
                    for (int dj = -1; dj <= 1; ++dj) {
                        int ni = i + di, nj = j + dj;
                        int pix = input_[(ni * width_) + nj];
                        sumgx += pix * gxkernel[di+1][dj+1];
                        sumgy += pix * gykernel[di+1][dj+1];
                    }
                }
                int magnitude = static_cast<int>(
                    std::sqrt(sumgx * sumgx + sumgy * sumgy)
                );
                output_[(i * width_) + j] = std::min(magnitude, 255);
            }
        }
    );
    return true;
}

bool zaytsev_d_sobel_tbb::TestTaskTBB::PostProcessingImpl() {
    std::ranges::copy(output_, reinterpret_cast<int *>(task_data->outputs[0]));
    return true;
}
\end{lstlisting}

\subsection{Гибрид MPI + TBB (\texttt{TestTaskALL})}

\begin{lstlisting}[language=C++, caption={Гибридная реализация MPI + TBB}]
bool zaytsev_d_sobel_all::TestTaskALL::RunImpl() {
    static constexpr int kGxkernel[3][3] = {
        {-1,  0,  1},
        {-2,  0,  2},
        {-1,  0,  1}
    };
    static constexpr int kGykernel[3][3] = {
        {-1, -2, -1},
        { 0,  0,  0},
        { 1,  2,  1}
    };

    int rows = height_ - 2, cols = width_ - 2;
    int total_pixels = rows * cols;
    int rank = world_.rank(), size = world_.size();

    int chunk = total_pixels / size;
    int rem = total_pixels % size;
    int start_idx = rank * chunk + std::min(rank, rem);
    int end_idx = start_idx + chunk + (rank < rem ? 1 : 0);

    tbb::parallel_for(
        tbb::blocked_range<int>(start_idx, end_idx),
        [&](const tbb::blocked_range<int> &r) {
            for (int idx = r.begin(); idx < r.end(); ++idx) {
                int i = 1 + (idx / cols), j = 1 + (idx % cols);
                int sumgx = 0, sumgy = 0;
                for (int di = -1; di <= 1; ++di) {
                    for (int dj = -1; dj <= 1; ++dj) {
                        int ni = i + di, nj = j + dj;
                        sumgx += input_[(ni * width_) + nj] * kGxkernel[di+1][dj+1];
                        sumgy += input_[(ni * width_) + nj] * kGykernel[di+1][dj+1];
                    }
                }
                int mag = static_cast<int>(
                    std::sqrt(sumgx * sumgx + sumgy * sumgy)
                );
                output_[(i * width_) + j] = std::min(mag, 255);
            }
        }
    );

    if (rank == 0) {
        std::vector<int> combined(output_.size());
        boost::mpi::reduce(world_, output_, combined, std::plus<>(), 0);
        output_.swap(combined);
    } else {
        boost::mpi::reduce(world_, output_, std::plus<>(), 0);
    }

    return true;
}
\end{lstlisting}

\end{document}
